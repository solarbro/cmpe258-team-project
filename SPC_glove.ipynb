{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "SPC_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Sno7IjUjr_"
      },
      "source": [
        "# SENTENCE PAIRS CLASSIFICATION: GLOVE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-eq9wVxUjsC"
      },
      "source": [
        "### 1. DATA EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-mHSLVwUjsD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Import packages for pre-processing\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Packages for pretrained glove and tfidf\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Cosine Similarity \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data Modelling\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SFdhCOxXcCP"
      },
      "source": [
        "# Download packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS1U7yFPVU6b"
      },
      "source": [
        "# Download dataset\n",
        "!wget \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\"\n",
        "# Extract\n",
        "!unzip \"./snli_1.0.zip\" -d \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEZrDwcdUjsE"
      },
      "source": [
        "# Load the SNLI dataset\n",
        "\n",
        "path = './data/snli_1.0/'\n",
        "train_data = pd.read_csv(path + 'snli_1.0_train.txt', sep=\"\\t\")\n",
        "train_data = pd.DataFrame(train_data, columns=['sentence1', 'sentence2', 'gold_label'])\n",
        "\n",
        "# Remove all the rows labeled as '-' and NaN values\n",
        "train_data = train_data.loc[train_data['gold_label'] != '-']\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# Create a subset of the data in order to run de model faster\n",
        "train_data = train_data.iloc[:10000,:]\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P00BkjyUjsF"
      },
      "source": [
        "# Test set\n",
        "test_data = pd.read_csv(path + 'snli_1.0_test.txt', sep=\"\\t\")\n",
        "test_data = pd.DataFrame(test_data, columns=['sentence1', 'sentence2', 'gold_label'])\n",
        "\n",
        "# Remove all the rows labeled as '-' and NaN values\n",
        "test_data = test_data.loc[test_data['gold_label'] != '-']\n",
        "test_data = test_data.dropna()\n",
        "\n",
        "# Create a subset of the data in order to run de model faster\n",
        "test_data = test_data.iloc[:2500,:]\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNJLMAPMUjsG"
      },
      "source": [
        "# Shuffle data\n",
        "train_data = train_data.sample(frac=1, random_state=203)\n",
        "\n",
        "# Size of the datasets \n",
        "print('Train Data size:', train_data.shape)\n",
        "print('Test Data size:', test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaN-40Y8UjsG"
      },
      "source": [
        "# Labels distributions\n",
        "print('==Labels train set:==\\n{}'.format(train_data['gold_label'].value_counts()))\n",
        "print('\\n==Labels test set:==\\n{}'.format(test_data['gold_label'].value_counts()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--0L9ZLzUjsH"
      },
      "source": [
        "### 2. DATA PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUeQB5WqUjsH"
      },
      "source": [
        "lemmat = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(sentence, lemma=False):    \n",
        "\n",
        "    # Step 1: Transform text to lower case, remove url, some punctuations and long repeated characters \n",
        "    sent = sentence.lower()\n",
        "    sent = re.sub(r'(http:)\\S+', r'', sent)\n",
        "    sent = re.sub(r'[\\.,`=_/#]', r' ', sent)\n",
        "    sent = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sent)\n",
        "\n",
        "    # Step 2: Find the tokens of each text and apply lemmatization\n",
        "    tokens = word_tokenize(sent)\n",
        "    if lemma: tokens = [lemmat.lemmatize(w) for w in tokens] \n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Data Cleaning in both sets\n",
        "train_data['clean_sent1'] = train_data['sentence1'].map(lambda x: clean_text(x, lemma=True))\n",
        "train_data['clean_sent2'] = train_data['sentence2'].map(lambda x: clean_text(x, lemma=True))\n",
        "\n",
        "test_data['clean_sent1'] = test_data['sentence1'].map(lambda x: clean_text(x, lemma=True))\n",
        "test_data['clean_sent2'] = test_data['sentence2'].map(lambda x: clean_text(x, lemma=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ1pLLNAUjsI"
      },
      "source": [
        "# Column with integer labels\n",
        "dict_label = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
        "train_data['int_label'] = train_data['gold_label'].map(lambda x: dict_label[x])\n",
        "test_data['int_label'] = test_data['gold_label'].map(lambda x: dict_label[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usKLCrGzUjsI"
      },
      "source": [
        "# Preview results after pre-processing\n",
        "train_data.iloc[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXr6YXlLUjsJ"
      },
      "source": [
        "### 3. TEXT VECTORIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcPzcsXYaNe"
      },
      "source": [
        "# Download glove vectors\n",
        "!wget \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "# Extract\n",
        "!unzip \"glove.6B.zip\" -d \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxYaF72UjsJ"
      },
      "source": [
        "# Loading the 300-dimension vectors\n",
        "path = \"data/glove.6B.300d.txt\"\n",
        "\n",
        "#glove_file = datapath(path)\n",
        "glove_vec_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
        "glove2word2vec(path, glove_vec_file)\n",
        "word_vectors = KeyedVectors.load_word2vec_format(glove_vec_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e5w8PXgUjsJ"
      },
      "source": [
        "# Size of the data vocabulary\n",
        "all_sent = np.concatenate([train_data['clean_sent1'].values,train_data['clean_sent2'].values], axis=0)\n",
        "words = []\n",
        "for i in all_sent:\n",
        "    tokens = word_tokenize(i.lower())\n",
        "    words.extend(tokens)    \n",
        "vocab = list(set(words))\n",
        "\n",
        "print('Size of the data vocab:', len(vocab))\n",
        "\n",
        "# Size of the GloVe vocabulary\n",
        "print(\"Size of GloVe's vocab:\", len(list(word_vectors.vocab)))\n",
        "\n",
        "# Number of words out of the GloVe vocabulary (OOV)\n",
        "out_vocab = []\n",
        "for i in vocab:\n",
        "    if i in word_vectors.vocab: continue\n",
        "    out_vocab.append(i)\n",
        "print('Number of OOV:', round(len(out_vocab)/len(vocab),2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMkLZonDUjsK"
      },
      "source": [
        "# Compute the embedding of a sentence by computing the average embedding of contained words\n",
        "def vectorize_sent(word_vectors, sent):\n",
        "    word_vecs = []\n",
        "    for token in sent.split():\n",
        "        if token not in word_vectors: continue\n",
        "        else:\n",
        "            word_vecs.append(word_vectors[token])\n",
        "\n",
        "    return np.mean(np.array(word_vecs),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8816l8MUjsK"
      },
      "source": [
        "# Vectorize the sentences\n",
        "X_train1 = np.array([vectorize_sent(word_vectors, sent) for sent in train_data['clean_sent1'].values])\n",
        "X_train2 = np.array([vectorize_sent(word_vectors, sent) for sent in train_data['clean_sent2'].values])\n",
        "\n",
        "X_test1 = np.array([vectorize_sent(word_vectors, sent) for sent in test_data['clean_sent1'].values])\n",
        "X_test2 = np.array([vectorize_sent(word_vectors, sent) for sent in test_data['clean_sent2'].values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgXI7p4BUjsK"
      },
      "source": [
        "print(X_train1.shape, X_train2.shape, X_test1.shape, X_test2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT3P6H-sUjsK"
      },
      "source": [
        "### 4. SENTENCE SIMILARITY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV8IEdA8UjsK"
      },
      "source": [
        "# Create a Dictionary of labels with its cosine values\n",
        "def dic_labels(x,y):\n",
        "    \n",
        "    dic = {\n",
        "    'neutral': [],\n",
        "    'entailment': [],\n",
        "    'contradiction': []\n",
        "    }\n",
        "    \n",
        "    for i,j in zip(x,y):\n",
        "        if j == 'entailment':\n",
        "            dic[j].append(i)\n",
        "        elif j == 'neutral':\n",
        "            dic[j].append(i)\n",
        "        else:\n",
        "            dic[j].append(i)\n",
        "    \n",
        "    return dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FOnyQEJUjsL"
      },
      "source": [
        "# Compute distance metrics\n",
        "cosine = lambda x,y: cosine_similarity(x.reshape(1,-1), y.reshape(1,-1)).flatten()[0]\n",
        "euclidean = lambda x,y: euclidean_distances(x.reshape(1,-1), y.reshape(1,-1)).flatten()[0]\n",
        "\n",
        "train_data['dist_cos'] = list(map(cosine, X_train1, X_train2))\n",
        "train_data['dist_euc'] = list(map(euclidean, X_train1, X_train2))\n",
        "\n",
        "dist_cos = dic_labels(train_data['dist_cos'].values, train_data['gold_label'].values)\n",
        "dist_euc = dic_labels(train_data['dist_euc'].values, train_data['gold_label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s78L7jLPUjsL"
      },
      "source": [
        "train_data.iloc[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR28p8p6UjsL"
      },
      "source": [
        "# Cosine scores distribution by labels\n",
        "metric = ['Cosine', 'Euclidean']\n",
        "results = [dist_cos, dist_euc]\n",
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "for i in range(0,2):\n",
        "    sb.distplot(results[i]['neutral'], hist=False, ax=ax[i])\n",
        "    sb.distplot(results[i]['entailment'], hist=False, color='green', ax=ax[i])\n",
        "    sb.distplot(results[i]['contradiction'], hist=False, color='red', ax=ax[i])\n",
        "    ax[i].set_title(metric[i] + ' Distribution by labels')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1jCBZzbUjsL"
      },
      "source": [
        "## 5. DATA MODELING. MULTI-CLASSIFICATION PROBLEM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNTIPzpsUjsM"
      },
      "source": [
        "# The approach used to represent the input is going to be subtraction (A sent_vector - B sent_vector)\n",
        "X_train = []\n",
        "for i in range(0,len(X_train1)):\n",
        "    subs = X_train1[i] - X_train2[i]\n",
        "    X_train.append(subs)\n",
        "    \n",
        "X_test = []\n",
        "for i in range(0,len(X_test1)):\n",
        "    subs = X_test1[i] - X_test2[i]\n",
        "    X_test.append(subs)\n",
        "\n",
        "y_train = np.array(train_data['int_label'])\n",
        "y_test = np.array(test_data['int_label'])\n",
        "\n",
        "print('X_train Shape:', len(X_train))\n",
        "print('y_train Shape:', y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZWsAmOrUjsM"
      },
      "source": [
        "class EstimatorSelectionHelper:\n",
        "    \n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys() # list of models' names\n",
        "        self.grid_searches = {} ## empty dictionary for the Grid.fit of each model\n",
        "    \n",
        "    def fit(self, X, y, **grid_kwargs):\n",
        "        for key in self.keys:\n",
        "            print('Running GridSearchCV for %s.' % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            grid_search = GridSearchCV(model, params, **grid_kwargs, cv=3)\n",
        "            grid_search.fit(X, y)\n",
        "            self.grid_searches[key] = grid_search ## fitting output from gird_search\n",
        "        print('Done.')\n",
        "    \n",
        "    def score_summary(self, sort_by='mean_test_score'):\n",
        "        frames = []\n",
        "        for name, grid_search in self.grid_searches.items():\n",
        "            frame = pd.DataFrame(grid_search.cv_results_) # The results for every combination of param\n",
        "            frame = frame.filter(regex='^(?!.*param_).*$') # remove columns about GRID parameters\n",
        "            frame['estimator'] = len(frame)*[name] # add the name of the model for every combo \n",
        "            frames.append(frame)\n",
        "        df = pd.concat(frames) # final dict of all the training that the grid model has done\n",
        "        \n",
        "        df = df.sort_values([sort_by], ascending=False)\n",
        "        df = df.reset_index()\n",
        "        df = df.drop(['rank_test_score', 'index'], 1)\n",
        "        \n",
        "        # Reorder the columns so estimator is the first one\n",
        "        columns = df.columns.tolist() \n",
        "        columns.remove('estimator') \n",
        "        columns = ['estimator']+columns \n",
        "        df = df[columns]\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-tCfs_uUjsM"
      },
      "source": [
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(n_jobs=-1),\n",
        "    'kNN': KNeighborsClassifier(n_jobs=-1),\n",
        "    'SVC': SVC(),\n",
        "    'RandomForestClassifier': RandomForestClassifier(n_jobs=-1)\n",
        "}\n",
        "\n",
        "params = {\n",
        "    'LogisticRegression': {'solver' : ['newton-cg', 'saga', 'sag'],\n",
        "                          'C' : np.logspace(-10, 0, 10)\n",
        "                          },\n",
        "    'kNN': {'n_neighbors' : [3, 5, 10],\n",
        "           'weights': [\"uniform\", \"distance\"]\n",
        "           },\n",
        "    'SVC': {'C': [0.1, 1, 10],\n",
        "              'gamma': ['auto', 'scale']\n",
        "           },\n",
        "    'RandomForestClassifier': {'n_estimators': [10,50,100],\n",
        "              'criterion': ['gini', 'entropy']\n",
        "            }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLKHan_7UjsM"
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "combined_results=pd.DataFrame()\n",
        "\n",
        "#X_train_vect, X_test_vect = vectorize_text(X_train, X_test, mode=vocab)\n",
        "helper = EstimatorSelectionHelper(models, params)\n",
        "#print(vocab)\n",
        "helper.fit(X_train, y_train, scoring='accuracy')\n",
        "results = helper.score_summary()\n",
        "#results['vectorizer'] = vocab\n",
        "combined_results = pd.concat([combined_results, results], ignore_index=True)\n",
        "\n",
        "print('Time in minutes:', (time.time() - t0)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qtw3K87UjsM"
      },
      "source": [
        "# Display the results dataframe\n",
        "results = results.sort_values(by='mean_test_score', ascending=False)\n",
        "results = results.reset_index(drop=True)\n",
        "cols = ['estimator', 'params', 'mean_test_score', 'mean_fit_time', 'mean_score_time']\n",
        "results = results[cols]\n",
        "results.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSODWWniUjsN"
      },
      "source": [
        "results.boxplot(column='mean_test_score',by='estimator', figsize=(15,5), rot=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZvVKAP9UjsN"
      },
      "source": [
        "### 6. ERROR ANALYSIS AND TEST RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAks4brZUjsN"
      },
      "source": [
        "# Predictions for the test set\n",
        "param = results.loc[0, 'params']\n",
        "top_model = SVC(**param)\n",
        "top_model.fit(X_train,y_train)\n",
        "y_pred = top_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfXahpEQUjsN"
      },
      "source": [
        "# Label the predictions with their correct name\n",
        "labels_name = {0:'entailment', 1:'neutral', 2:'contradiction'}\n",
        "label_pred = [labels_name[i] for i in y_pred] \n",
        "\n",
        "# Create a big data frame with the true labels and their predictions\n",
        "final_df = test_data[['sentence1', 'sentence2', 'gold_label']]\n",
        "final_df['predictions'] = label_pred \n",
        "final_df.head(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbtwDYUCUjsN"
      },
      "source": [
        "# Confusion Matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "metrics.confusion_matrix(y_test, y_pred) # Rows = true_labels and Col = predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnAuGmE9UjsN"
      },
      "source": [
        "# Compare svm accuracy to the baseline method ('majority class')\n",
        "baseline_acc = 854/2500\n",
        "diff = 0.62/baseline_acc\n",
        "print('Improvement of SVM: ',round(diff,2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}